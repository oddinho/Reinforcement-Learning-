{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd3c730",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m \n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbandit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bandits_one, Bandits_two, Bandits_three\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\pythonlek\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:998\u001b[39m\n\u001b[32m    992\u001b[39m     _log.debug(\u001b[33m'\u001b[39m\u001b[33mloaded rc file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m, fname)\n\u001b[32m    994\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[32m    997\u001b[39m rcParamsDefault = _rc_params_in_file(\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     \u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatplotlibrc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    999\u001b[39m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[32m   1000\u001b[39m     transform=\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[32m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line.startswith(\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[32m   1001\u001b[39m     fail_on_error=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1002\u001b[39m rcParamsDefault._update_raw(rcsetup._hardcoded_defaults)\n\u001b[32m   1003\u001b[39m rcParamsDefault._ensure_has_backend()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\pythonlek\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py:603\u001b[39m, in \u001b[36m_get_data_path\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_data_path\u001b[39m(*args):\n\u001b[32m    598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[32m    600\u001b[39m \n\u001b[32m    601\u001b[39m \u001b[33;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data_path\u001b[49m(), *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\pythonlek\\.venv\\Lib\\site-packages\\matplotlib\\_api\\__init__.py:218\u001b[39m, in \u001b[36mcaching_module_getattr.<locals>.__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name].\u001b[34m__get__\u001b[39m(instance)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    219\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from bandit import Bandits_one, Bandits_two, Bandits_three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edf90c",
   "metadata": {},
   "source": [
    "Epsilon greedy algo implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c54fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(mab_env, T=1000, epsilon=0.1):\n",
    "    k = mab_env.k                  #k represents num arms in each MAB_env, otherwise, init E[R|a]=0, N_i = 0\n",
    "    estimated_exp_reward = np.zeros(k, dtype=float)   # estimated mean reward per arm\n",
    "    num_actions = np.zeros(k, dtype=int)     # pull counts per arm\n",
    "\n",
    "    actions = np.zeros(T, dtype=int)\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "\n",
    "    #this is sort of equiv to num simulations\n",
    "    for t in range(T):\n",
    "        # explore vs exploit\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(k)\n",
    "        else:\n",
    "            action = int(np.argmax(estimated_exp_reward))\n",
    "\n",
    "        observation, reward, terminated, truncated, info = mab_env.step(action)\n",
    "\n",
    "        # incremental mean update\n",
    "        num_actions[action] += 1\n",
    "        estimated_exp_reward[action] += (reward - estimated_exp_reward[action]) / num_actions[action]\n",
    "\n",
    "        actions[t] = action\n",
    "        rewards[t] = reward\n",
    "\n",
    "    return estimated_exp_reward, num_actions, actions, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b4440",
   "metadata": {},
   "source": [
    "Decaying Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c020d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_epsilon_greedy(mab_env, T = 1000, epsilon = 0.1, alpha = 0.1):\n",
    "    k = mab_env.k # same as above, represents num arms in each bandit env\n",
    "    estimated_exp_reward = np.zeros(k, dtype=float)\n",
    "    num_actions = np.zeros(k, dtype=int)  \n",
    "\n",
    "    actions = np.zeros(T, dtype=int)\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "\n",
    "    for t in range(T):\n",
    "        # explore vs exploit\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(k)\n",
    "        else:\n",
    "            action = int(np.argmax(estimated_exp_reward))\n",
    "        \n",
    "\n",
    "        observation, reward, terminated, truncated, info = mab_env.step(action)\n",
    "\n",
    "        # incremental mean update\n",
    "        num_actions[action] += 1\n",
    "        estimated_exp_reward[action] += (reward - estimated_exp_reward[action]) / num_actions[action]\n",
    "\n",
    "        actions[t] = action\n",
    "        rewards[t] = reward\n",
    "\n",
    "        #algo similar to normal epsilon greedy, so have to add alpha-decay \n",
    "\n",
    "        epsilon = epsilon * alpha\n",
    "\n",
    "    return estimated_exp_reward, num_actions, actions, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386152ec",
   "metadata": {},
   "source": [
    "UCB Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a326260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(mab_env, T= 1000, c = 2):\n",
    "    k = mab_env.k\n",
    "    estimated_exp_reward = np.zeros(k, dtype=float)   # estimated mean reward per arm\n",
    "    num_actions = np.zeros(k, dtype=int)     # pull counts per arm  \n",
    "\n",
    "    actions = np.zeros(T, dtype=int)\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        Ucb_values = np.zeros(k, dtype=float)\n",
    "        for action in range(k): \n",
    "            if num_actions[action] == 0:\n",
    "                Ucb_values[action] = float('inf')  # ensure each arm is selected at least once\n",
    "            else:\n",
    "                Ucb_values[action] = estimated_exp_reward[action] + c * np.sqrt(np.log(t) / num_actions[action])\n",
    "\n",
    "        action = int(np.argmax(Ucb_values))\n",
    "        observation, reward, terminated, truncated, info = mab_env.step(action) \n",
    "\n",
    "        # incremental mean update\n",
    "        num_actions[action] += 1\n",
    "        estimated_exp_reward[action] += (reward - estimated_exp_reward[action]) / num_actions[action]\n",
    "\n",
    "        actions[t-1] = action   \n",
    "        rewards[t-1] = reward\n",
    "\n",
    "    return estimated_exp_reward, num_actions, actions, rewards  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d00feb",
   "metadata": {},
   "source": [
    "1000 runs of each algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb93e831",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Bandits_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m T = \u001b[32m1000\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# epsilon-greedy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m env = \u001b[43mBandits_one\u001b[49m()\n\u001b[32m      5\u001b[39m estimated_reward, num_actions, actions, rewards = epsilon_greedy(env, T=T, epsilon=\u001b[32m0.1\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEpsilon-greedy best arm:\u001b[39m\u001b[33m\"\u001b[39m, np.argmax(estimated_reward))\n",
      "\u001b[31mNameError\u001b[39m: name 'Bandits_one' is not defined"
     ]
    }
   ],
   "source": [
    "T = 1000\n",
    "\n",
    "# epsilon-greedy\n",
    "env = Bandits_one()\n",
    "estimated_reward, num_actions, actions, rewards = epsilon_greedy(env, T=T, epsilon=0.1)\n",
    "print(\"Epsilon-greedy best arm:\", np.argmax(estimated_reward))\n",
    "\n",
    "# decaying epsilon-greedy\n",
    "env = Bandits_one()\n",
    "Qd, Nd, Ad, Rd = decaying_epsilon_greedy(env, T=T, epsilon=1.0, alpha=0.995)\n",
    "print(\"Decaying epsilon-greedy best arm:\", np.argmax(Qd))\n",
    "\n",
    "# UCB\n",
    "env = Bandits_one()\n",
    "Qu, Nu, Au, Ru = UCB(env, T=T, c=2.0)\n",
    "print(\"UCB best arm:\", np.argmax(Qu))\n",
    "\n",
    "# true optimal\n",
    "env = Bandits_one()\n",
    "print(\"True optimal arm:\", env.get_optimal_action(), \"True means:\", env.means)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
