{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d591d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Sum of Rewards P(R+S):\n",
      "Sum of Rewards     Probability    \n",
      "-----------------------------------\n",
      "$0                 0.1050 (10.50%)\n",
      "$5                 0.2125 (21.25%)\n",
      "$10                0.2850 (28.50%)\n",
      "$15                0.1775 (17.75%)\n",
      "$20                0.1225 (12.25%)\n",
      "$25                0.0500 (5.00%)\n",
      "$30                0.0425 (4.25%)\n",
      "$40                0.0050 (0.50%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1.10\n",
    "# Slot machine reward distribution\n",
    "rewards = [0, 5, 10, 20]  # Dollar amounts\n",
    "probabilities = [0.35, 0.3, 0.25, 0.1]  # Probabilities\n",
    "\n",
    "#Second slot machine with new reward distribution\n",
    "rewards2 = [0, 5, 10, 20]  # Dollar\n",
    "probabilities2 = [0.3, 0.35, 0.3, 0.05]  # New Probabilities\n",
    "\n",
    "# Calculate the distribution of the sum\n",
    "sum_distribution = {}\n",
    "for r_reward, r_prob in zip(rewards, probabilities):\n",
    "    for s_reward, s_prob in zip(rewards2, probabilities2):\n",
    "        total = r_reward + s_reward\n",
    "        prob = r_prob * s_prob\n",
    "        if total not in sum_distribution:\n",
    "            sum_distribution[total] = 0\n",
    "        sum_distribution[total] += prob\n",
    "\n",
    "# Sort by total reward\n",
    "sorted_sum = sorted(sum_distribution.items())\n",
    "sum_totals = [x[0] for x in sorted_sum]\n",
    "sum_probs = [x[1] for x in sorted_sum]\n",
    "\n",
    "print(\"Distribution of the Sum of Rewards P(R+S):\")\n",
    "print(f\"{'Sum of Rewards':<18} {'Probability':<15}\")\n",
    "print(\"-\" * 35)\n",
    "for total, prob in sorted_sum:\n",
    "    print(f\"${total:<17} {prob:.4f} ({prob:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f6149",
   "metadata": {},
   "source": [
    "Exercise 1.14 (*)\n",
    "\n",
    "Since both slot machines are independent, the probability of the sum of both machines being more than 20 is:\n",
    "P(R+S > 20 | R = 10) = P(S > 10) = P(S = 20) = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320eb40",
   "metadata": {},
   "source": [
    "Exercise 4.2\n",
    "\n",
    "States: we can map the surfaces that needs to be cleaned as an x,y grid where each cell represents a part of the room. The state would then be the current position of the robot on this grid and which cells have been cleaned.\n",
    "\n",
    "Actions: The actions would be the possible movements the robot can make (up, down, left, right) and an action to vacuum the current cell.\n",
    "\n",
    "Rewards: The robot would recieve a positive reward for vacuuming a cell that is dirty and a small negative reward for each movement to encourage efficiency. As it is a house cleaning robot, we can also impose a negative reward for crashing into obstacles or walls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f02fb",
   "metadata": {},
   "source": [
    "Exercise 4.6\n",
    "\n",
    "The reason for the competing robot crashing more, but performing better in other ways, can be that the competing firm has prioritized performance (time, efficient sweeping) over safety (avoiding crashes). \n",
    "It could for example be that the competing robot drives faster, which increases the chance of crashing, but also allows it to finish the task quicker.\n",
    "For sure the competing robot has a less negative reward for crashing, which leads it to adapt policies that crash more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a49f5b",
   "metadata": {},
   "source": [
    "Exercise 5\n",
    "\n",
    "We find the choice to use RL in this scenario inappropriate as it would face some ethical and practical challenges.\n",
    "Firstly, RL typically requires a large number of interactions with the environment to learn an effective policy. This is ethically problematic as it would risk the first applicants to not be considered fairly, as the system would be learning and making mistakes during this period.\n",
    "\n",
    "Secondly, the recruitment process often involves subjective judgments and qualitative factors that are difficult to represent and incorporate into a reward function. E.g. it would be difficult to design well-structured reward functions that capture all relevant aspects of a candidate's potential. One could argue that you can create some kind of efficiency measure, but the data collection for this would likely take a long time and be very noisy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b268adb",
   "metadata": {},
   "source": [
    "Exercise 6. MABs\n",
    "\n",
    "Giving three different vaccines to the same population and gradually estimating which vaccine is the most efficient based on a relevant reward measure. To get these estimates you would need to use exploration vs exploitation strategies to make sure you get good estimates of each vaccine's efficiency. A good strategy would ensure that you gradually would shift towards giving the most efficient vaccine to the majority of the population and thus maximize your goal.\n",
    "\n",
    "This MAB approach can be more sensible than a RCT (randomized control trial) as it is adaptive and will gradually give more people the best vaccine. This can be crucial in for example a pandemic situation where you want to maximize the efficiency of the vaccine rollout as quickly as possible and not wait for the results of a full RCT before changing strategy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
