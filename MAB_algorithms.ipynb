{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from bandit import Bandits_one, Bandits_two, Bandits_three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edf90c",
   "metadata": {},
   "source": [
    "Epsilon greedy algo implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c54fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(mab_env, T=1000, epsilon=0.1):\n",
    "    k = mab_env.k                  #k represents num arms in each MAB_env, otherwise, init E[R|a]=0, N_i = 0\n",
    "    estimated_exp_reward = np.zeros(k, dtype=float)   # estimated mean reward per arm\n",
    "    num_actions = np.zeros(k, dtype=int)     # pull counts per arm\n",
    "\n",
    "    actions = np.zeros(T, dtype=int)\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "\n",
    "    #this is sort of equiv to num simulations\n",
    "    for t in range(T):\n",
    "        # explore vs exploit\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(k)\n",
    "        else:\n",
    "            action = int(np.argmax(estimated_exp_reward))\n",
    "\n",
    "        observation, reward, terminated, truncated, info = mab_env.step(action)\n",
    "\n",
    "        # incremental mean update\n",
    "        num_actions[action] += 1\n",
    "        estimated_exp_reward[action] += (reward - estimated_exp_reward[action]) / num_actions[action]\n",
    "\n",
    "        actions[t] = action\n",
    "        rewards[t] = reward\n",
    "\n",
    "    return estimated_exp_reward, num_actions, actions, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b4440",
   "metadata": {},
   "source": [
    "Decaying Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_epsilon_greedy(mab_env, T = 1000, epsilon = 0.1, alpha = 0.1):\n",
    "    k = mab_env.k # same as above, represents num arms in each bandit env\n",
    "    estimated_exp_reward = np.zeros(k, dtype=float)\n",
    "    num_actions = np.zeros(k, dtype=int)  \n",
    "\n",
    "    actions = np.zeros(T, dtype=int)\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "\n",
    "    for t in range(T):\n",
    "        # explore vs exploit\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(k)\n",
    "        else:\n",
    "            action = int(np.argmax(estimated_exp_reward))\n",
    "        \n",
    "\n",
    "        observation, reward, terminated, truncated, info = mab_env.step(action)\n",
    "\n",
    "        # incremental mean update\n",
    "        num_actions[action] += 1\n",
    "        estimated_exp_reward[action] += (reward - estimated_exp_reward[action]) / num_actions[action]\n",
    "\n",
    "        actions[t] = action\n",
    "        rewards[t] = reward\n",
    "\n",
    "        #algo similar to normal epsilon greedy, so have to add alpha-decay \n",
    "\n",
    "        epsilon = epsilon * alpha\n",
    "\n",
    "    return estimated_exp_reward, num_actions, actions, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386152ec",
   "metadata": {},
   "source": [
    "UCB Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a326260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(mab_env, T= 1000, c = 2):\n",
    "    k = mab_env.k\n",
    "    estimated_exp_reward = np.zeros(k, dtype=float)   # estimated mean reward per arm\n",
    "    num_actions = np.zeros(k, dtype=int)     # pull counts per arm  \n",
    "\n",
    "    actions = np.zeros(T, dtype=int)\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        Ucb_values = np.zeros(k, dtype=float)\n",
    "        for action in range(k): \n",
    "            if num_actions[action] == 0:\n",
    "                Ucb_values[action] = float('inf')  # ensure each arm is selected at least once\n",
    "            else:\n",
    "                Ucb_values[action] = estimated_exp_reward[action] + c * np.sqrt(np.log(t) / num_actions[action])\n",
    "\n",
    "        action = int(np.argmax(Ucb_values))\n",
    "        observation, reward, terminated, truncated, info = mab_env.step(action) \n",
    "\n",
    "        # incremental mean update\n",
    "        num_actions[action] += 1\n",
    "        estimated_exp_reward[action] += (reward - estimated_exp_reward[action]) / num_actions[action]\n",
    "\n",
    "        actions[t-1] = action   \n",
    "        rewards[t-1] = reward\n",
    "\n",
    "    return estimated_exp_reward, num_actions, actions, rewards  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d00feb",
   "metadata": {},
   "source": [
    "1000 runs of each algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb93e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy best arm: 2\n",
      "Decaying epsilon-greedy best arm: 2\n",
      "UCB best arm: 2\n",
      "True optimal arm: 2 True means: [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "T = 1000\n",
    "\n",
    "# epsilon-greedy\n",
    "env = Bandits_one()\n",
    "Qe, Ne, Ae, Re = epsilon_greedy(env, T=T, epsilon=0.1)\n",
    "print(\"Epsilon-greedy best arm:\", np.argmax(Qe))\n",
    "\n",
    "# decaying epsilon-greedy\n",
    "env = Bandits_one()\n",
    "Qd, Nd, Ad, Rd = decaying_epsilon_greedy(env, T=T, epsilon=1.0, alpha=0.995)\n",
    "print(\"Decaying epsilon-greedy best arm:\", np.argmax(Qd))\n",
    "\n",
    "# UCB\n",
    "env = Bandits_one()\n",
    "Qu, Nu, Au, Ru = UCB(env, T=T, c=2.0)\n",
    "print(\"UCB best arm:\", np.argmax(Qu))\n",
    "\n",
    "# true optimal\n",
    "env = Bandits_one()\n",
    "print(\"True optimal arm:\", env.get_optimal_action(), \"True means:\", env.means)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
